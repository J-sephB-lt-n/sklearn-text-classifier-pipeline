{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries #\n",
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "import random\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "# import 3rd party packages #\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.lines\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import sklearn.base\n",
    "import sklearn.datasets\n",
    "import sklearn.discriminant_analysis\n",
    "import sklearn.ensemble\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.neural_network\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.tree\n",
    "import sklearn.compose\n",
    "import tensorflow_hub\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class trainConfig:\n",
    "    \"\"\"Specifies training process settings\"\"\"\n",
    "\n",
    "    k_cross_valid_folds: int = 10  # number of cross-validation folds\n",
    "    use_n_cores: int = (\n",
    "        6  # passed to n_jobs arg of sklearn.model_selection.cross_validate\n",
    "    )\n",
    "    train_undersample_frac: float = (\n",
    "        0.5  # I only used 50% of the training data because I was sick of waiting for models to finish training\n",
    "    )\n",
    "\n",
    "\n",
    "training_config: trainConfig = trainConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data #\n",
    "train_data_dict = sklearn.datasets.fetch_20newsgroups(\n",
    "    subset=\"train\",\n",
    "    shuffle=True,\n",
    "    random_state=69,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    # return_X_y=True,\n",
    ")\n",
    "\n",
    "test_data_dict = sklearn.datasets.fetch_20newsgroups(\n",
    "    subset=\"test\",\n",
    "    shuffle=True,\n",
    "    random_state=69,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    # return_X_y=True,\n",
    ")\n",
    "\n",
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "\n",
    "n_train_samples = int(\n",
    "    len(train_data_dict[\"data\"]) * training_config.train_undersample_frac\n",
    ")\n",
    "x_train = train_data_dict[\"data\"][:n_train_samples]\n",
    "x_test = test_data_dict[\"data\"]\n",
    "\n",
    "y_labels_train = [\n",
    "    train_data_dict[\"target_names\"][y_idx]\n",
    "    for y_idx in train_data_dict[\"target\"][:n_train_samples]\n",
    "]\n",
    "y_labels_test = [\n",
    "    test_data_dict[\"target_names\"][y_idx] for y_idx in test_data_dict[\"target\"]\n",
    "]\n",
    "\n",
    "label_encoder.fit(y_labels_train)\n",
    "y_codes_train = label_encoder.transform(y_labels_train)\n",
    "y_codes_test = label_encoder.transform(y_labels_test)\n",
    "\n",
    "del train_data_dict, test_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a random example #\n",
    "random_idx = random.randint(0, len(x_train))\n",
    "print(\"<\", y_labels_train[random_idx], \">\")\n",
    "print(x_train[random_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify tokenization strategy for bag-of-words (n-gram) features #\n",
    "tokenizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    strip_accents=\"ascii\",\n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 3),\n",
    "    max_features=10_000,  # only use the top \"max_features\" features\n",
    "    max_df=0.1,  # don't consider n-grams more common that this\n",
    ")\n",
    "\n",
    "# create bag-of-words (n-gram) features #\n",
    "tokenizer.fit(x_train)\n",
    "x_train_bag_of_words: scipy.sparse._csr.csr_matrix = tokenizer.transform(x_train)\n",
    "x_test_bag_of_words: scipy.sparse._csr.csr_matrix = tokenizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load document embedding model #\n",
    "universal_sentence_encoder = tensorflow_hub.load(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    ")\n",
    "\n",
    "# create document embeddings #\n",
    "x_train_embeddings: np.ndarray = universal_sentence_encoder(x_train).numpy()\n",
    "x_test_embeddings: np.ndarray = universal_sentence_encoder(x_test).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine bag-of-words and embeddings into a single dataset #\n",
    "# this requires the (dense) embeddings to be converted to a sparse matrix #\n",
    "# (this seemed wiser than converting the very sparse data to dense) #\n",
    "x_train_embeddings_sparse = scipy.sparse.csr_matrix(x_train_embeddings)\n",
    "x_train_bagofwords_and_embeddings_sparse = scipy.sparse.hstack(\n",
    "    (x_train_bag_of_words, x_train_embeddings_sparse)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Experiments to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify models to fit #\n",
    "models: dict[str, Any] = {\n",
    "    \"adaboost\": sklearn.ensemble.AdaBoostClassifier(),\n",
    "    \"decision_tree\": sklearn.tree.DecisionTreeClassifier(),\n",
    "    \"extremely_random_trees\": sklearn.ensemble.ExtraTreesClassifier(),\n",
    "    # \"gbm\": sklearn.ensemble.GradientBoostingClassifier(), # too slow\n",
    "    # \"hist_gbm\": HistGradientBoostingClassifier(), # doesn't support sparse X\n",
    "    \"logistic_regression\": sklearn.linear_model.LogisticRegression(\n",
    "        penalty=None,\n",
    "        max_iter=1_000,\n",
    "    ),\n",
    "    # \"naive_bayes\": sklearn.naive_bayes.MultinomialNB(), # requires non-negative X\n",
    "    \"neural_net\": sklearn.neural_network.MLPClassifier(\n",
    "        hidden_layer_sizes=(50, 30, 10, 10), activation=\"relu\", max_iter=5_000\n",
    "    ),\n",
    "    # \"qda\": sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(), # doesn't support sparse X\n",
    "    \"random_forest\": sklearn.ensemble.RandomForestClassifier(),\n",
    "    \"xg_boost\": xgboost.XGBClassifier(objective=\"multi:softmax\"),\n",
    "}\n",
    "\n",
    "models[\"stacked_ensemble\"] = sklearn.ensemble.StackingClassifier(\n",
    "    estimators=[\n",
    "        (\n",
    "            model_name,\n",
    "            sklearn.base.clone(models[model_name]),\n",
    "        )\n",
    "        for model_name in models.keys()\n",
    "    ],\n",
    "    final_estimator=xgboost.XGBClassifier(objective=\"multi:softmax\"),\n",
    "    cv=5,\n",
    "    stack_method=\"predict_proba\",\n",
    "    n_jobs=1,\n",
    "    passthrough=False,  # False=train final_estimator on estimators preds only (i.e. exclude original covariates)\n",
    ")\n",
    "\n",
    "# specify datasets to fit #\n",
    "datasets_to_fit: list[str] = [\n",
    "    \"bag_of_words_only\",\n",
    "    \"embeddings_only\",\n",
    "    \"bag_of_words_and_embeddings\",\n",
    "]\n",
    "\n",
    "# list out all model/dataset combinations #\n",
    "experiments_to_run = tuple(itertools.product(datasets_to_fit, models.keys()))\n",
    "print(\"-- All Experiments To Run --\")\n",
    "for experiment_num, experiment_contents in enumerate(experiments_to_run):\n",
    "    dataset_name, model_name = experiment_contents\n",
    "    print(\n",
    "        f\"experiment_id=[{experiment_num}]  dataset=[{dataset_name}], model=[{model_name}]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric calculated in each holdout fold in order to evaluate overall model performance on that fold is called [roc_auc_ovr](https://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "\n",
    "This metric is generated by calculating a [ROC AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) score for each unique class (using a [one-vs-rest](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#one-vs-rest-multiclass-roc) approach), and then taking a macro-average (unweighted mean) over these scores to obtain a single final [roc_auc_ovr](https://scikit-learn.org/stable/modules/model_evaluation.html) score.\n",
    "\n",
    "Note that this approach may be a bad idea if there is severe class-imbalance in the data, and can also hide poor model performance on specific classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run k-Fold Cross Validation\n",
    "\n",
    "(to identify the best-performing model(s) and feature engineering strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run k-fold cross-validation on all model/dataset combinations #\n",
    "k_fold_cv_results: dict[str, Any] = {}\n",
    "for experiment_num, experiment_contents in enumerate(experiments_to_run):\n",
    "    start_time = time.perf_counter()\n",
    "    dataset_name, model_name = experiment_contents\n",
    "    print(\n",
    "        f\"\"\"\n",
    "    Running experiment        {experiment_num}\n",
    "    Dataset:                  {dataset_name}\n",
    "    Model:                    {model_name} \n",
    "\"\"\"\n",
    "    )\n",
    "    match dataset_name:\n",
    "        case \"bag_of_words_only\":\n",
    "            temp_x: scipy.sparse._csr.csr_matrix = x_train_bag_of_words\n",
    "        case \"embeddings_only\":\n",
    "            temp_x: np.ndarray = x_train_embeddings\n",
    "        case \"bag_of_words_and_embeddings\":\n",
    "            temp_x: scipy.sparse._csr.csr_matrix = (\n",
    "                x_train_bagofwords_and_embeddings_sparse\n",
    "            )\n",
    "    k_fold_cv_results[\n",
    "        (model_name, dataset_name)\n",
    "    ] = sklearn.model_selection.cross_validate(\n",
    "        estimator=sklearn.base.clone(models[model_name]),\n",
    "        X=temp_x,\n",
    "        y=y_codes_train,\n",
    "        scoring=\"roc_auc_ovr\",  # see https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "        cv=training_config.k_cross_valid_folds,\n",
    "        return_train_score=False,\n",
    "        return_estimator=False,\n",
    "        n_jobs=training_config.use_n_cores,\n",
    "    )\n",
    "    minutes_elapsed = (time.perf_counter() - start_time) / 60\n",
    "    test_score_per_label = k_fold_cv_results[(model_name, dataset_name)].get(\n",
    "        \"test_score\"\n",
    "    )\n",
    "    print(\n",
    "        \"   Scores on holdout folds: \"\n",
    "        + f\"(mean={np.mean(test_score_per_label):.3f}) (min={min(test_score_per_label):.3f}) (max={max(test_score_per_label):.3f}) scores: \"\n",
    "        + \", \".join([f\"{test_score:.2f}\" for test_score in test_score_per_label])\n",
    "    )\n",
    "    print(f\"    ...done ({minutes_elapsed:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate summary statistics for each model/dataset combination cross-validation result #\n",
    "for key, value in k_fold_cv_results.items():\n",
    "    value[\"mean_test_score\"] = value[\"test_score\"].mean()\n",
    "    value[\"min_test_score\"] = value[\"test_score\"].min()\n",
    "    value[\"max_test_score\"] = value[\"test_score\"].max()\n",
    "\n",
    "# sort results by mean test score #\n",
    "k_fold_cv_results = {\n",
    "    k: v\n",
    "    for k, v in sorted(\n",
    "        k_fold_cv_results.items(),\n",
    "        key=lambda item: item[1].get(\"mean_test_score\"),\n",
    "        reverse=True,\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot k-Fold Cross-Validation Performance for each Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "# draw in each group mean #\n",
    "plt.scatter(\n",
    "    y=[f\"model={key[0]}, dataset={key[1]}\" for key in k_fold_cv_results.keys()],\n",
    "    x=[value.get(\"mean_test_score\") for value in k_fold_cv_results.values()],\n",
    "    color=\"red\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "# draw in performance on each fold #\n",
    "plt.scatter(\n",
    "    y=tuple(\n",
    "        itertools.chain.from_iterable(\n",
    "            [\n",
    "                [f\"model={key[0]}, dataset={key[1]}\"] * trainConfig.k_cross_valid_folds\n",
    "                for key in k_fold_cv_results.keys()\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    x=tuple(\n",
    "        itertools.chain.from_iterable(\n",
    "            [value.get(\"test_score\").tolist() for value in k_fold_cv_results.values()]\n",
    "        )\n",
    "    ),\n",
    "    marker=\"|\",\n",
    ")\n",
    "plt.xlabel(\"Macro-Averaged ROC AUC (OVR)\")\n",
    "plt.grid()\n",
    "plt.title(\n",
    "    f\"Per-Fold Model Performance (Macro-Averaged ROC AUC (OVR)) ({training_config.k_cross_valid_folds} Folds)\"\n",
    ")\n",
    "# draw legend #\n",
    "plt.legend(\n",
    "    handles=[\n",
    "        matplotlib.lines.Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            marker=\"o\",\n",
    "            color=\"red\",\n",
    "            alpha=0.5,\n",
    "            label=\"Average (Mean)\",\n",
    "            markerfacecolor=\"red\",\n",
    "            markersize=10,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Final Chosen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = models.get(\"stacked_ensemble\")\n",
    "final_model.fit(X=x_train_bagofwords_and_embeddings_sparse, y=y_codes_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Prediction Pipeline\n",
    "\n",
    "(contains both feature engineering and trained model within a single useable/saveable/loadable scikit-learn **pipeline** object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model prediction pipeline #\n",
    "embedder = sklearn.preprocessing.FunctionTransformer(\n",
    "    lambda x: universal_sentence_encoder(x), feature_names_out=\"one-to-one\"\n",
    ")\n",
    "prediction_pipeline = sklearn.pipeline.Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"feature_prep\",\n",
    "            sklearn.pipeline.FeatureUnion(\n",
    "                [(\"bag_of_words\", tokenizer), (\"document_embedding\", embedder)]\n",
    "            ),\n",
    "        ),\n",
    "        (\"model\", final_model),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Final Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on test data #\n",
    "test_pred_codes = prediction_pipeline.predict(X=x_test)\n",
    "test_preds_proba = prediction_pipeline.predict_proba(X=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix on test data #\n",
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "confusion_matrix: np.ndarray = sklearn.metrics.confusion_matrix(\n",
    "    y_true=y_codes_test, y_pred=test_pred_codes\n",
    ")\n",
    "confusion_matrix_display = sklearn.metrics.ConfusionMatrixDisplay(\n",
    "    confusion_matrix, display_labels=label_encoder.classes_\n",
    ")\n",
    "confusion_matrix_display.plot()\n",
    "fig = confusion_matrix_display.ax_.get_figure()\n",
    "fig.set_figwidth(9)\n",
    "fig.set_figheight(9)\n",
    "plt.title(\"Confusion Matrix (Test Set Predictions)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model test set performance (accuracy) for different confidence thresholds #\n",
    "# i.e. if we consider model predictions with a low probability value as \"model doesn't know\",\n",
    "#       then what accuracy can we achieve over only \"high confidence\" predictions\n",
    "#       (and for what proportion of the data do we have \"high confidence\" predictions)\n",
    "test_max_pred_proba = test_preds_proba.max(axis=1)\n",
    "prob_thresholds = np.arange(test_preds_proba.min(), test_preds_proba.max(), 0.01)\n",
    "test_accuracy: list[float] = []\n",
    "percent_of_obs: list[float] = []\n",
    "for prob_thresh in prob_thresholds:\n",
    "    idx_mask = test_max_pred_proba > prob_thresh\n",
    "    temp_y_true = y_codes_test[idx_mask]\n",
    "    temp_y_pred = test_pred_codes[idx_mask]\n",
    "    test_accuracy.append((temp_y_pred == temp_y_true).sum() / len(temp_y_true))\n",
    "    percent_of_obs.append(idx_mask.sum() / len(test_max_pred_proba))\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(prob_thresholds, test_accuracy, label=\"model accuracy\")\n",
    "plt.plot(prob_thresholds, percent_of_obs, label=\"% of total data\")\n",
    "plt.xticks(np.arange(0.0, 1.1, 0.1))\n",
    "plt.yticks(np.arange(0.0, 1.1, 0.1))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title(\n",
    "    \"Test Set Performance: Model Accuracy under Different Minimum Required Confidence Thresholds\"\n",
    ")\n",
    "plt.xlabel(\"Confidence Threshold (Minimum Acceptable Probability of Predicted Label)\")\n",
    "plt.ylabel(\"Model Accuracy / % of Total Data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
